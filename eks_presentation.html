<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced AWS EKS Workshop - Production Guide</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
        }
        
        .presentation-container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            box-shadow: 0 20px 60px rgba(0,0,0,0.1);
            border-radius: 15px;
            overflow: hidden;
        }
        
        .slide {
            min-height: 90vh;
            padding: 40px 60px;
            display: none;
            position: relative;
            overflow-y: auto;
        }
        
        .slide.active {
            display: block;
        }
        
        .slide h1 {
            color: #2c3e50;
            font-size: 3em;
            margin-bottom: 30px;
            text-align: center;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        }
        
        .slide h2 {
            color: #34495e;
            font-size: 2.2em;
            margin-bottom: 25px;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        
        .slide h3 {
            color: #2980b9;
            font-size: 1.5em;
            margin-bottom: 15px;
        }
        
        .slide p, .slide li {
            font-size: 1.1em;
            line-height: 1.6;
            margin-bottom: 15px;
        }
        
        .slide ul {
            padding-left: 30px;
        }
        
        .slide li {
            margin-bottom: 10px;
        }
        
        .code-block {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            margin: 20px 0;
            overflow-x: auto;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }
        
        .architecture-diagram {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border: 2px solid #3498db;
            border-radius: 15px;
            padding: 30px;
            margin: 30px 0;
            text-align: center;
            font-family: monospace;
            font-size: 0.9em;
            line-height: 1.8;
        }
        
        .cost-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        
        .cost-table th, .cost-table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        .cost-table th {
            background: linear-gradient(135deg, #3498db, #2980b9);
            color: white;
            font-weight: bold;
        }
        
        .cost-table tr:nth-child(even) {
            background-color: #f8f9fa;
        }
        
        .navigation {
            position: fixed;
            bottom: 30px;
            right: 30px;
            z-index: 1000;
        }
        
        .nav-btn {
            background: linear-gradient(135deg, #3498db, #2980b9);
            color: white;
            border: none;
            padding: 12px 20px;
            margin: 0 5px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 16px;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }
        
        .nav-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0,0,0,0.3);
        }
        
        .slide-counter {
            position: fixed;
            top: 30px;
            right: 30px;
            background: rgba(52, 73, 94, 0.8);
            color: white;
            padding: 10px 15px;
            border-radius: 20px;
            font-size: 14px;
            z-index: 1000;
        }
        
        .highlight-box {
            background: linear-gradient(135deg, #e8f5e8, #c8e6c9);
            border-left: 5px solid #4caf50;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .warning-box {
            background: linear-gradient(135deg, #fff3e0, #ffe0b2);
            border-left: 5px solid #ff9800;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .network-flow {
            background: #f8f9fa;
            border: 2px solid #007bff;
            border-radius: 10px;
            padding: 25px;
            margin: 25px 0;
            font-family: monospace;
            font-size: 1em;
            line-height: 2;
        }
        
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .metric-card {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }
        
        .metric-card h4 {
            margin-top: 0;
            font-size: 1.2em;
        }
    </style>
</head>
<body>
    <div class="presentation-container">
        <div class="slide-counter">
            <span id="current-slide">1</span> / <span id="total-slides">15</span>
        </div>

        <!-- Slide 1: Title -->
        <div class="slide active">
            <h1>ğŸš€ Advanced AWS EKS Workshop</h1>
            <h2 style="text-align: center; color: #3498db;">Production-Grade Kubernetes on AWS</h2>
            <div style="text-align: center; margin-top: 50px;">
                <h3>Complete Guide to:</h3>
                <ul style="font-size: 1.3em; text-align: left; display: inline-block;">
                    <li>Production EKS Architecture & Multi-AZ HA Setup</li>
                    <li>Advanced Networking & Pod Communication</li>
                    <li>Security Best Practices & Implementation</li>
                    <li>Monitoring with Datadog Integration</li>
                    <li>Cost Optimization (EC2 vs Fargate)</li>
                    <li>Hands-on Code Examples & Best Practices</li>
                </ul>
            </div>
        </div>

        <!-- Slide 2: Production EKS Architecture -->
        <div class="slide">
            <h2>ğŸ—ï¸ Production-Grade EKS Architecture</h2>
            <div class="architecture-diagram">
                <h3>Multi-AZ High Availability Setup</h3>
                <pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AWS Region (us-east-1)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    VPC (10.0.0.0/16)                   â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  AZ-1a          AZ-1b          AZ-1c                   â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚ â”‚Public   â”‚    â”‚Public   â”‚    â”‚Public   â”‚              â”‚   â”‚
â”‚  â”‚ â”‚Subnet   â”‚    â”‚Subnet   â”‚    â”‚Subnet   â”‚              â”‚   â”‚
â”‚  â”‚ â”‚10.0.1.0 â”‚    â”‚10.0.2.0 â”‚    â”‚10.0.3.0 â”‚              â”‚   â”‚
â”‚  â”‚ â”‚   /24   â”‚    â”‚   /24   â”‚    â”‚   /24   â”‚              â”‚   â”‚
â”‚  â”‚ â”‚         â”‚    â”‚         â”‚    â”‚         â”‚              â”‚   â”‚
â”‚  â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚              â”‚   â”‚
â”‚  â”‚ â”‚ â”‚ ALB â”‚ â”‚    â”‚ â”‚ ALB â”‚ â”‚    â”‚ â”‚ ALB â”‚ â”‚              â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚              â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚ â”‚Private  â”‚    â”‚Private  â”‚    â”‚Private  â”‚              â”‚   â”‚
â”‚  â”‚ â”‚Subnet   â”‚    â”‚Subnet   â”‚    â”‚Subnet   â”‚              â”‚   â”‚
â”‚  â”‚ â”‚10.0.11.0â”‚    â”‚10.0.12.0â”‚    â”‚10.0.13.0â”‚              â”‚   â”‚
â”‚  â”‚ â”‚   /24   â”‚    â”‚   /24   â”‚    â”‚   /24   â”‚              â”‚   â”‚
â”‚  â”‚ â”‚         â”‚    â”‚         â”‚    â”‚         â”‚              â”‚   â”‚
â”‚  â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚    â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚    â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚              â”‚   â”‚
â”‚  â”‚ â”‚â”‚Worker â”‚â”‚    â”‚â”‚Worker â”‚â”‚    â”‚â”‚Worker â”‚â”‚              â”‚   â”‚
â”‚  â”‚ â”‚â”‚Nodes  â”‚â”‚    â”‚â”‚Nodes  â”‚â”‚    â”‚â”‚Nodes  â”‚â”‚              â”‚   â”‚
â”‚  â”‚ â”‚â”‚& Pods â”‚â”‚    â”‚â”‚& Pods â”‚â”‚    â”‚â”‚& Pods â”‚â”‚              â”‚   â”‚
â”‚  â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚    â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚    â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚              â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚ â”‚Database â”‚    â”‚Database â”‚    â”‚Database â”‚              â”‚   â”‚
â”‚  â”‚ â”‚Subnet   â”‚    â”‚Subnet   â”‚    â”‚Subnet   â”‚              â”‚   â”‚
â”‚  â”‚ â”‚10.0.21.0â”‚    â”‚10.0.22.0â”‚    â”‚10.0.23.0â”‚              â”‚   â”‚
â”‚  â”‚ â”‚   /24   â”‚    â”‚   /24   â”‚    â”‚   /24   â”‚              â”‚   â”‚
â”‚  â”‚ â”‚  â”Œâ”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”  â”‚              â”‚   â”‚
â”‚  â”‚ â”‚  â”‚RDSâ”‚  â”‚    â”‚  â”‚RDSâ”‚  â”‚    â”‚  â”‚RDSâ”‚  â”‚              â”‚   â”‚
â”‚  â”‚ â”‚  â””â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”˜  â”‚              â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                EKS Control Plane                       â”‚   â”‚
â”‚  â”‚              (Managed by AWS)                          â”‚   â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”                      â”‚   â”‚
â”‚  â”‚     â”‚ API â”‚    â”‚ETCD â”‚    â”‚Schedâ”‚                      â”‚   â”‚
â”‚  â”‚     â”‚ Srv â”‚    â”‚     â”‚    â”‚uler â”‚                      â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                </pre>
            </div>
            <div class="highlight-box">
                <h3>Key Architecture Components:</h3>
                <ul>
                    <li><strong>Control Plane:</strong> Fully managed by AWS across multiple AZs</li>
                    <li><strong>Worker Nodes:</strong> Distributed across 3 AZs for HA</li>
                    <li><strong>Load Balancer:</strong> Application Load Balancer in public subnets</li>
                    <li><strong>Database:</strong> RDS with Multi-AZ deployment</li>
                </ul>
            </div>
        </div>

        <!-- Slide 3: Network Traffic Flow -->
        <div class="slide">
            <h2>ğŸŒ Network Traffic Flow: Internet to Pods</h2>
            <div class="network-flow">
                <h3>Request Flow Architecture</h3>
                <pre>
Internet Request â†’ Route 53 â†’ CloudFront (Optional) â†’ ALB
    â†“
ALB Target Groups â†’ EKS Service (ClusterIP/NodePort)
    â†“
kube-proxy (iptables rules) â†’ Pod (via CNI Plugin)
    â†“
Container Application
                </pre>
            </div>
            
            <h3>Detailed Traffic Flow Steps:</h3>
            <div class="code-block">
              <code class="language-text"> 
                <pre>
1. External Request Processing:
   Internet â†’ Route 53 DNS Resolution â†’ CloudFront CDN (optional)
   â†’ Application Load Balancer (ALB) in Public Subnets

2. ALB to Kubernetes Service:
   ALB â†’ Target Groups â†’ Worker Nodes (NodePort/Instance targets)
   â†’ kube-proxy â†’ Service Discovery â†’ Pod Selection

3. Pod-to-Pod Communication:
   Source Pod â†’ CNI Plugin (AWS VPC CNI) â†’ VPC Routing
   â†’ Destination Pod's Network Interface

4. Pod-to-AWS Services:
   Pod â†’ VPC Endpoints (optional) â†’ AWS Services (S3, RDS, etc.)
   OR Pod â†’ Internet Gateway â†’ AWS Services
                </pre>
              </code>
            </div>

            <div class="highlight-box">
                <h3>Network Components:</h3>
                <ul>
                    <li><strong>AWS VPC CNI:</strong> Native VPC networking for pods</li>
                    <li><strong>Service Mesh:</strong> Istio/Linkerd for advanced traffic management</li>
                    <li><strong>Network Policies:</strong> Kubernetes native or Calico for security</li>
                    <li><strong>VPC Endpoints:</strong> Private connectivity to AWS services</li>
                </ul>
            </div>
        </div>

        <!-- Slide 4: Pod Communication Patterns -->
        <div class="slide">
            <h2>ğŸ”— Pod-to-Pod Communication</h2>
            
            <div class="architecture-diagram">
                <h3>Pod Networking with AWS VPC CNI</h3>
                <pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Worker Node (EC2)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  kubelet                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚              Pod Network                    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  Pod A  â”‚    â”‚  Pod B  â”‚    â”‚  Pod C  â”‚ â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚10.0.1.10â”‚    â”‚10.0.1.11â”‚    â”‚10.0.1.12â”‚ â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚         â”‚    â”‚         â”‚    â”‚         â”‚ â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ veth0   â”‚    â”‚ veth0   â”‚    â”‚ veth0   â”‚ â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚   â”‚
â”‚  â”‚  â”‚       â”‚             â”‚             â”‚        â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚          â”‚             â”‚             â”‚            â”‚   â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚      â”‚          Bridge cni0              â”‚        â”‚   â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â”‚                        â”‚                          â”‚   â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚      â”‚         Primary ENI               â”‚        â”‚   â”‚
â”‚  â”‚      â”‚        (10.0.1.5)                â”‚        â”‚   â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                 VPC Subnet                         â”‚    â”‚
â”‚  â”‚                (10.0.1.0/24)                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                </pre>
            </div>

            <h3>Pod Communication Mechanisms:</h3>
            <div class="code-block">
              <code class="language-text"> 
                <pre>
# 1. Same Node Communication (Fastest)
Pod A (10.0.1.10) â†’ Bridge cni0 â†’ Pod B (10.0.1.11)

# 2. Different Node Communication
Pod A â†’ ENI â†’ VPC Route Table â†’ Target ENI â†’ Pod C

# 3. Service-based Communication
Pod A â†’ Service ClusterIP â†’ kube-proxy â†’ Target Pod(s)

# 4. DNS Resolution
Pod A â†’ CoreDNS Service â†’ Service Discovery â†’ Target Pod
                </pre>
              </code>
            </div>

            <div class="highlight-box">
                <h3>AWS VPC CNI Benefits:</h3>
                <ul>
                    <li><strong>Native VPC IPs:</strong> Pods get actual VPC IP addresses</li>
                    <li><strong>High Performance:</strong> No overlay network overhead</li>
                    <li><strong>Security Groups:</strong> Native AWS security group support</li>
                    <li><strong>Network Policies:</strong> Fine-grained traffic control</li>
                </ul>
            </div>
        </div>

        <!-- Slide 5: EKS Cluster Creation with eksctl -->
        <div class="slide">
            <h2>âš™ï¸ Creating Production EKS Cluster with eksctl</h2>
            
            <h3>1. Prerequisites & Installation:</h3>
            <div class="code-block">
              <code class="language-bash"> 
                <pre>
# Install eksctl
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin

# Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Install AWS CLI v2
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
                </pre>
              </code>
            </div>

            <h3>2. Production Cluster Configuration:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Create cluster.yaml for production setup
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: production-eks-cluster
  region: us-east-1
  version: "1.31"

# VPC Configuration
vpc:
  cidr: "10.0.0.0/16"
  nat:
    gateway: HighlyAvailable
  clusterEndpoints:
    privateAccess: true
    publicAccess: true
    publicAccessCIDRs: ["0.0.0.0/0"]

# IAM Configuration
iam:
  withOIDC: true
  serviceAccounts:
  - metadata:
      name: aws-load-balancer-controller
      namespace: kube-system
    wellKnownPolicies:
      awsLoadBalancerController: true
  - metadata:
      name: cluster-autoscaler
      namespace: kube-system
    wellKnownPolicies:
      autoScaler: true
  - metadata:
      name: ebs-csi-controller-sa
      namespace: kube-system
    wellKnownPolicies:
      ebsCSIController: true

# Managed Node Groups
managedNodeGroups:
- name: production-workers
  instanceType: m5.xlarge
  minSize: 2
  maxSize: 10
  desiredCapacity: 3
  availabilityZones: ["us-east-1a", "us-east-1b", "us-east-1c"]
  
  # EBS Volume Configuration
  volumeSize: 100
  volumeType: gp3
  volumeEncrypted: true
  
  # Security Configuration
  privateNetworking: true
  
  # Labels and Tags
  labels:
    node-type: "production-worker"
    environment: "production"
  
  tags:
    Environment: production
    ManagedBy: eksctl
    
  # Instance Metadata Service
  instanceMetadataServicePolicy: required

# Fargate Profiles for serverless workloads
fargateProfiles:
- name: fp-default
  selectors:
  - namespace: default
    labels:
      compute-type: fargate
- name: fp-kube-system
  selectors:
  - namespace: kube-system
    labels:
      compute-type: fargate

# Add-ons
addons:
- name: vpc-cni
  version: latest
  configurationValues: |-
    env:
      ENABLE_PREFIX_DELEGATION: "true"
      ENABLE_POD_ENI: "true"
- name: coredns
  version: latest
- name: kube-proxy
  version: latest
- name: aws-ebs-csi-driver
  version: latest
  serviceAccountRoleARN: arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole

# Logging
logging:
  enable:
    - api
    - audit
    - authenticator
    - controllerManager
    - scheduler
  logRetentionInDays: 7
                </pre>
              </code>
            </div>
        </div>

        <!-- Slide 6: Cluster Creation Commands -->
        <div class="slide">
            <h2>ğŸš€ EKS Cluster Deployment Commands</h2>
            
            <h3>3. Create the Cluster:</h3>
            <div class="code-block">
              <code class="language-bash"> 
                <pre>
# Create cluster using config file
eksctl create cluster -f cluster.yaml

# Alternative: Create cluster with CLI parameters
eksctl create cluster \
  --name production-eks-cluster \
  --region us-east-1 \
  --version 1.31 \
  --nodegroup-name production-workers \
  --node-type m5.xlarge \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 10 \
  --managed \
  --enable-ssm \
  --with-oidc \
  --ssh-access \
  --ssh-public-key your-key-name
                </pre>
              </code>
            </div>

            <h3>4. Post-Deployment Configuration:</h3>
            <div class="code-block">
              <code class="language-bash"> 
                <pre>
# Update kubeconfig
aws eks update-kubeconfig --region us-east-1 --name production-eks-cluster

# Verify cluster connection
kubectl get nodes -o wide

# Install AWS Load Balancer Controller
helm repo add eks https://aws.github.io/eks-charts
helm repo update

helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=production-eks-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller

# Install Cluster Autoscaler
kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml

kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict="false"

kubectl -n kube-system edit deployment.apps/cluster-autoscaler
# Add: --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/production-eks-cluster
                </pre>
              </code>
            </div>

            <h3>5. Verify Installation:</h3>
            <div class="code-block">
              <code class="language-bash"> 
                <pre>
# Check eksctl version
eksctl version
              
# Check cluster status
eksctl get cluster --name production-eks-cluster --region us-east-1

# Check node groups
eksctl get nodegroup --cluster production-eks-cluster --region us-east-1

# Check add-ons
eksctl get addon --cluster production-eks-cluster --region us-east-1

# Verify AWS Load Balancer Controller
kubectl get deployment -n kube-system aws-load-balancer-controller

# Check cluster info
kubectl cluster-info
# Check nodes
kubectl get nodes -o wide
# Check namespaces
kubectl get namespaces
# Check pods in kube-system
kubectl get pods -n kube-system -o wide
# Check services
kubectl get services -n kube-system 
                </pre>
              </code>
            </div>
        </div>

        <!-- Slide 7: Sample Application Deployment -->
        <div class="slide">
            <h2>ğŸ“± Sample Application Deployment</h2>
            
            <h3>1. Sample Microservices Application:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Create namespace
apiVersion: v1
kind: Namespace
metadata:
  name: sample-app
  labels:
    name: sample-app
---
# Deployment for frontend service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: sample-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: frontend
        image: nginx:1.21
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
---
# Service for frontend
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  namespace: sample-app
spec:
  selector:
    app: frontend
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
---
# Backend API deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
  namespace: sample-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: backend-api
  template:
    metadata:
      labels:
        app: backend-api
    spec:
      containers:
      - name: backend-api
        image: httpd:2.4
        ports:
        - containerPort: 80
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: database-url
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
# Service for backend
apiVersion: v1
kind: Service
metadata:
  name: backend-service
  namespace: sample-app
spec:
  selector:
    app: backend-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
---
# Application Load Balancer Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: sample-app-ingress
  namespace: sample-app
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:ACCOUNT_ID:certificate/CERTIFICATE_ID
spec:
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: backend-service
            port:
              number: 80
                </pre>
              </code>
            </div>

            <h3>2. Database Secret Configuration:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Create database credentials secret
apiVersion: v1
kind: Secret
metadata:
  name: db-credentials
  namespace: sample-app
type: Opaque
data:
  database-url: cG9zdGdyZXNxbDovL3VzZXI6cGFzc3dvcmRAcmRzLWVuZHBvaW50OjU0MzIvZGJuYW1l
  username: dXNlcm5hbWU=
  password: cGFzc3dvcmQ=

# Apply the manifests
kubectl apply -f namespace.yaml
kubectl apply -f secrets.yaml
kubectl apply -f deployments.yaml
kubectl apply -f services.yaml
kubectl apply -f ingress.yaml
                </pre>
              </code>
            </div>

            <h3>3. Horizontal Pod Autoscaler:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# HPA for frontend
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-hpa
  namespace: sample-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
---
# HPA for backend
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-hpa
  namespace: sample-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend-api
  minReplicas: 2
  maxReplicas: 15
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 85
                </pre>
              </code>
            </div>
        </div>

        <!-- Slide 8: Security Best Practices -->
        <div class="slide">
            <h2>ğŸ” EKS Security Best Practices</h2>
            
            <h3>1. Identity and Access Management:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Create IAM Role for EKS Service Account
apiVersion: iam.amazonaws.com/v1
kind: Role
metadata:
  name: S3AccessRole
  namespace: sample-app 
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/S3AccessRole
rules:
- apiGroups: [""]
  resources: ["secrets", "configmaps"]  
  verbs: ["get", "list", "create", "update", "delete"]
- apiGroups: ["s3.amazonaws.com"]
  resources: ["buckets"]
  verbs: ["get", "list", "create", "update", "delete"]

# Create Kubernetes Service Account
apiVersion: v1 
kind: ServiceAccount
metadata:
  name: s3-access-sa
  namespace: sample-app
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/S3AccessRole
---
# Create RBAC for administrators
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: admin-role
rules:
- apiGroups: [""] 
  resources: ["*"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: ["apps", "extensions", "networking.k8s.io"]
  resources: ["*"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["*"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-binding
subjects:
- kind: User
  name: admin-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: admin-role
  apiGroup: rbac.authorization.k8s.io 


# Create RBAC for developers
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: developer-role
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: ["extensions", "networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: developer-binding
subjects:
- kind: User
  name: developer-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: developer-role
  apiGroup: rbac.authorization.k8s.io

# AWS IAM Role for Service Account (IRSA)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: s3-access-sa
  namespace: sample-app
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/S3AccessRole
                </pre>
              </code> 
            </div>

            <h3>2. Network Security Policies:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Network Policy - Deny all ingress by default
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: sample-app
spec:
  podSelector: {}
  policyTypes:
  - Ingress
---
# Allow frontend to backend communication
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-backend
  namespace: sample-app
spec:
  podSelector:
    matchLabels:
      app: backend-api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 80
---
# Allow ingress controller to frontend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-to-frontend
  namespace: sample-app
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 80
                </pre>
              </code>
            </div>

            <h3>3. Pod Security Standards:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Pod Security Policy (using Pod Security Standards)
apiVersion: v1
kind: Namespace
metadata:
  name: secure-app
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

# Security Context in Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-app
  namespace: secure-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: secure-app
  template:
    metadata:
      labels:
        app: secure-app
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 3000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: app
        image: nginx:1.21
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: var-cache
          mountPath: /var/cache/nginx
        - name: var-run
          mountPath: /var/run
      volumes:
      - name: tmp
        emptyDir: {}
      - name: var-cache
        emptyDir: {}
      - name: var-run
        emptyDir: {}
                </pre>
              </code>
            </div>
        </div>

        <!-- Slide 9: Security Best Practices Continued -->
        <div class="slide">
            <h2>ğŸ›¡ï¸ Advanced Security Configuration</h2>

            <h3>4. Secrets Management with External Secrets Operator:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Install External Secrets Operator
helm repo add external-secrets https://charts.external-secrets.io
helm install external-secrets external-secrets/external-secrets -n external-secrets-system --create-namespace

# SecretStore for AWS Secrets Manager
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secrets-manager
  namespace: sample-app
spec:
  provider:
    aws:
      service: SecretsManager
      region: us-east-1
      auth:
        jwt:
          serviceAccountRef:
            name: external-secrets-sa
---
# ExternalSecret to pull from AWS Secrets Manager
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: database-credentials
  namespace: sample-app
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secrets-manager
    kind: SecretStore
  target:
    name: db-credentials
    creationPolicy: Owner
  data:
  - secretKey: username
    remoteRef:
      key: prod/db/credentials
      property: username
  - secretKey: password
    remoteRef:
      key: prod/db/credentials
      property: password
                </pre>
              </code>
            </div>

            <h3>5. Image Scanning and Admission Controllers:</h3>
            <div class="code-block">
              <code class="language-bash"> 
                <pre>
# Enable ECR image scanning
aws ecr put-image-scanning-configuration \
  --repository-name my-repo \
  --image-scanning-configuration scanOnPush=true  
# Install OPA Gatekeeper for policy enforcement
helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts
helm install gatekeeper gatekeeper/gatekeeper \
  --namespace gatekeeper-system \
  --create-namespace \
  --set enableCRDValidation=true \
  --set auditInterval=24h \
  --set webhookTimeoutSeconds=10
# Install Kyverno for policy management
helm repo add kyverno https://kyverno.github.io/kyverno
helm install kyverno kyverno/kyverno \
  --namespace kyverno-system \
  --create-namespace \
  --set admissionController.enabled=true \
  --set metrics.enabled=true \
  --set webhookTimeoutSeconds=10

# Install Falco for runtime security
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm install falco falcosecurity/falco \
  --namespace falco-system \
  --create-namespace \
  --set falco.grpc.enabled=true \
  --set falco.grpcOutput.enabled=true

# OPA Gatekeeper for policy enforcement
kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.14/deploy/gatekeeper.yaml

                </pre>
              </code>
            </div>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>

# Constraint Template for required labels
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        openAPIV3Schema:
          type: object
          properties:
            labels:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels
        violation[{"msg": msg}] {
          required := input.parameters.labels
          provided := input.review.object.metadata.labels
          missing := required[_]
          not provided[missing]
          msg := sprintf("Missing required label: %v", [missing])
        }
---
# Constraint requiring specific labels
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: must-have-environment
spec:
  match:
    kinds:
      - apiGroups: ["apps"]
        kinds: ["Deployment"]
  parameters:
    labels: ["environment", "team", "version"]
                </pre>
              </code>
            </div>

            <h3>6. Cluster Hardening Checklist:</h3>
            <div class="warning-box">
                <h4>Essential Security Measures:</h4>
                <ul>
                    <li><strong>API Server:</strong> Restrict public access, enable audit logging</li>
                    <li><strong>Node Security:</strong> Use AWS Systems Manager Session Manager instead of SSH</li>
                    <li><strong>Container Images:</strong> Use ECR with image scanning enabled</li>
                    <li><strong>Network:</strong> Implement VPC security groups and NACLs</li>
                    <li><strong>Encryption:</strong> Enable encryption at rest and in transit</li>
                    <li><strong>Monitoring:</strong> CloudTrail, GuardDuty, and Security Hub integration</li>
                </ul>
            </div>
        </div>

        <!-- Slide 10: Datadog Integration -->
        <div class="slide">
            <h2>ğŸ“Š Datadog Monitoring Integration</h2>
            
            <h3>1. Install Datadog Agent using Helm:</h3>
            <div class="code-block">
              <code class="language-bash"> 
                <pre>
# Add Datadog Helm repository
helm repo add datadog https://helm.datadoghq.com
helm repo update

# Create Datadog namespace
kubectl create namespace datadog

# Create secret for Datadog API key
kubectl create secret generic datadog-secret --from-literal api-key=YOUR_DATADOG_API_KEY -n datadog

# Install Datadog Agent with comprehensive monitoring
helm install datadog-agent datadog/datadog \
  --namespace datadog \
  --set datadog.site='datadoghq.com' \
  --set datadog.apiKeyExistingSecret='datadog-secret' \
  --set datadog.logs.enabled=true \
  --set datadog.logs.containerCollectAll=true \
  --set datadog.apm.enabled=true \
  --set datadog.processAgent.enabled=true \
  --set datadog.systemProbe.enableTCPQueueLength=true \
  --set datadog.systemProbe.enableOOMKill=true \
  --set datadog.networkMonitoring.enabled=true \
  --set clusterAgent.enabled=true \
  --set clusterAgent.metricsProvider.enabled=true \
  --set clusterAgent.admissionController.enabled=true \
  --set clusterAgent.admissionController.mutateUnlabelled=true
                </pre>
              </code>
            </div>

            <h3>2. Advanced Datadog Configuration:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# values.yaml for Datadog Helm chart
datadog:
  site: "datadoghq.com"
  apiKeyExistingSecret: "datadog-secret"
  
  # Logging configuration
  logs:
    enabled: true
    containerCollectAll: true
    containerExcludeByLogs:
      - "image:.*fluent.*"
      - "image:.*telegraf.*"
    
  # APM and tracing
  apm:
    enabled: true
    port: 8126
    useHostPort: true
    
  # Process monitoring
  processAgent:
    enabled: true
    processCollection: true
    
  # Network monitoring
  networkMonitoring:
    enabled: true
    
  # Custom metrics
  confd:
    redis.yaml: |-
      ad_identifiers:
        - redis
      init_config:
      instances:
        - host: "%%host%%"
          port: "%%port%%"
    
    postgres.yaml: |-
      ad_identifiers:
        - postgres
      init_config:
      instances:
        - host: "%%host%%"
          port: "%%port%%"
          username: datadog
          password: "%%env_POSTGRES_PASSWORD%%"

# Cluster Agent configuration
clusterAgent:
  enabled: true
  replicas: 2
  
  # External metrics for HPA
  metricsProvider:
    enabled: true
    useDatadogMetrics: true
    
  # Admission controller for automatic instrumentation
  admissionController:
    enabled: true
    mutateUnlabelled: true
    
agents:
  # Resource limits
  containers:
    agent:
      resources:
        requests:
          cpu: 200m
          memory: 256Mi
        limits:
          cpu: 200m
          memory: 256Mi
    
    processAgent:
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 100m
          memory: 200Mi
    
    traceAgent:
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 100m
          memory: 200Mi
                </pre>
              </code>
            </div>
        </div>

        <!-- Slide 11: Datadog Metrics Collection -->
        <div class="slide">
            <h2>ğŸ“ˆ Datadog P1 Metrics Collection</h2>
            
            <div class="metrics-grid">
                <div class="metric-card">
                    <h4>ğŸš€ Application Metrics</h4>
                    <ul style="text-align: left; font-size: 0.9em;">
                        <li>Request Rate (RPS)</li>
                        <li>Response Time (P50, P95, P99)</li>
                        <li>Error Rate (%)</li>
                        <li>Throughput</li>
                    </ul>
                </div>
                
                <div class="metric-card">
                    <h4>ğŸ—ï¸ Infrastructure Metrics</h4>
                    <ul style="text-align: left; font-size: 0.9em;">
                        <li>CPU Utilization (%)</li>
                        <li>Memory Usage (MB/GB)</li>
                        <li>Disk I/O (IOPS)</li>
                        <li>Network Traffic (MB/s)</li>
                    </ul>
                </div>
                
                <div class="metric-card">
                    <h4>â˜¸ï¸ Kubernetes Metrics</h4>
                    <ul style="text-align: left; font-size: 0.9em;">
                        <li>Pod Status & Restarts</li>
                        <li>Node Health & Capacity</li>
                        <li>Service Discovery</li>
                        <li>Resource Quotas</li>
                    </ul>
                </div>
                
                <div class="metric-card">
                    <h4>ğŸ” Security Metrics</h4>
                    <ul style="text-align: left; font-size: 0.9em;">
                        <li>Failed Authentication</li>
                        <li>RBAC Violations</li>
                        <li>Network Policy Blocks</li>
                        <li>Container Vulnerabilities</li>
                    </ul>
                </div>
            </div>

            <h3>Custom Metrics Collection Configuration:</h3>
            <div class="code-block">
              <code class="language-python"> 
                <pre>
# Configure custom metrics in application

# Python application example
import ddtrace
from ddtrace import patch_all

# Patch all supported libraries
patch_all()

# Initialize tracer
ddtrace.config.service = 'sample-backend'
ddtrace.config.env = 'production'
ddtrace.config.version = '2.1.0'

# Custom metrics
from datadog import initialize, statsd

initialize(
    api_key='YOUR_API_KEY',
    app_key='YOUR_APP_KEY'
)

# Send custom metrics
statsd.increment('myapp.page_views')
statsd.histogram('myapp.response_time', response_time)
statsd.gauge('myapp.queue_size', queue_size)

                </pre>
              </code>
            </div>

            <h3>Kubernetes Annotations for Auto-Discovery:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Add annotations to deployment for automatic metric collection
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-app
  namespace: sample-app
spec:
  template:
    metadata:
      annotations:
        ad.datadoghq.com/redis.check_names: '["redisdb"]'
        ad.datadoghq.com/redis.init_configs: '[{}]'
        ad.datadoghq.com/redis.instances: '[{"host": "%%host%%", "port": "6379", "password": "%%env_REDIS_PASSWORD%%"}]'
        ad.datadoghq.com/redis.logs: '[{"source": "redis", "service": "redis"}]'
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        env:
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: redis-secret
              key: password
              </pre>
              </code>
            </div>
        </div>

        <!-- Slide 12: Cost Analysis -->
        <div class="slide">
            <h2>ğŸ’° AWS EKS Cost Analysis: EC2 vs Fargate</h2>
            
            <h3>EKS Control Plane Pricing (Fixed):</h3>
            <table class="cost-table">
                <tr>
                    <th>Component</th>
                    <th>Price</th>
                    <th>Notes</th>
                </tr>
                <tr>
                    <td>EKS Control Plane</td>
                    <td>$0.10/hour</td>
                    <td>$73/month per cluster</td>
                </tr>
            </table>

            <h3>EC2 Worker Nodes Cost Analysis:</h3>
            <table class="cost-table">
                <tr>
                    <th>Instance Type</th>
                    <th>vCPUs</th>
                    <th>Memory (GB)</th>
                    <th>On-Demand ($/hour)</th>
                    <th>Monthly Cost</th>
                    <th>Reserved (1yr)</th>
                </tr>
                <tr>
                    <td>t3.medium</td>
                    <td>2</td>
                    <td>4</td>
                    <td>$0.0416</td>
                    <td>$30.36</td>
                    <td>$19.71</td>
                </tr>
                <tr>
                    <td>t3.large</td>
                    <td>2</td>
                    <td>8</td>
                    <td>$0.0832</td>
                    <td>$60.74</td>
                    <td>$39.42</td>
                </tr>
                <tr>
                    <td>m5.large</td>
                    <td>2</td>
                    <td>8</td>
                    <td>$0.096</td>
                    <td>$70.08</td>
                    <td>$45.47</td>
                </tr>
                <tr>
                    <td>m5.xlarge</td>
                    <td>4</td>
                    <td>16</td>
                    <td>$0.192</td>
                    <td>$140.16</td>
                    <td>$90.94</td>
                </tr>
                <tr>
                    <td>m5.2xlarge</td>
                    <td>8</td>
                    <td>32</td>
                    <td>$0.384</td>
                    <td>$280.32</td>
                    <td>$181.87</td>
                </tr>
                <tr>
                    <td>c5.large</td>
                    <td>2</td>
                    <td>4</td>
                    <td>$0.085</td>
                    <td>$62.05</td>
                    <td>$40.29</td>
                </tr>
            </table>

            <h3>Fargate Pricing Model:</h3>
            <table class="cost-table">
                <tr>
                    <th>Resource</th>
                    <th>Price per hour</th>
                    <th>Notes</th>
                </tr>
                <tr>
                    <td>vCPU</td>
                    <td>$0.04048/vCPU</td>
                    <td>Minimum 0.25 vCPU</td>
                </tr>
                <tr>
                    <td>Memory</td>
                    <td>$0.004445/GB</td>
                    <td>Minimum 0.5 GB</td>
                </tr>
            </table>

            <h3>Cost Comparison Scenarios:</h3>
            <div class="code-block">
              <code class="language-text"> 
                <pre>
# Scenario 1: Small Application (0.5 vCPU, 1GB RAM)
Fargate Cost per hour: (0.5 * $0.04048) + (1 * $0.004445) = $0.024685
Fargate Monthly Cost: $0.024685 * 24 * 30 = $17.77

# Scenario 2: Medium Application (2 vCPU, 4GB RAM)  
Fargate Cost per hour: (2 * $0.04048) + (4 * $0.004445) = $0.09874
Fargate Monthly Cost: $0.09874 * 24 * 30 = $71.09

# EC2 t3.medium (2 vCPU, 4GB): $30.36/month
# But you can run multiple pods on same instance

# Scenario 3: High-CPU Application (4 vCPU, 8GB RAM)
Fargate Cost per hour: (4 * $0.04048) + (8 * $0.004445) = $0.19748
Fargate Monthly Cost: $0.19748 * 24 * 30 = $142.19

# EC2 m5.xlarge (4 vCPU, 16GB): $140.16/month
# Better value with higher memory and multi-pod capacity


                </pre>
              </code>
            </div>
        </div>

        <!-- Slide 13: Cost Optimization -->
        <div class="slide">
            <h2>ğŸ’¡ Cost Optimization Strategies</h2>
            
            <h3>1. Right-Sizing and Auto-Scaling:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Cluster Autoscaler configuration for cost optimization
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/production-eks-cluster
        - --balance-similar-node-groups
        - --scale-down-enabled=true
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
        - --scale-down-delay-after-delete=10s
        - --scale-down-delay-after-failure=3m
        - --scale-down-utilization-threshold=0.5

# Vertical Pod Autoscaler for right-sizing
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: frontend-vpa
  namespace: sample-app
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: frontend
      maxAllowed:
        cpu: 1
        memory: 2Gi
      minAllowed:
        cpu: 100m
        memory: 128Mi

                </pre>
              </code>
            </div>

            <h3>2. Spot Instances Integration:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Mixed instance node group with Spot instances
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

managedNodeGroups:
- name: spot-workers
  instanceTypes: ["m5.large", "m5.xlarge", "m4.large", "m4.xlarge"]
  spot: true
  minSize: 1
  maxSize: 10
  desiredCapacity: 3
  volumeSize: 100
  ssh:
    allow: true
    publicKeyName: your-key
  labels:
    node-type: spot-worker
    lifecycle: Ec2Spot
  taints:
  - key: spotInstance
    value: "true"
    effect: NoSchedule
  tags:
    nodegroup-type: spot-workers

# Deployment with spot instance toleration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: batch-processor
  namespace: sample-app
spec:
  replicas: 3
  template:
    spec:
      tolerations:
      - key: spotInstance
        operator: Equal
        value: "true"
        effect: NoSchedule
      nodeSelector:
        lifecycle: Ec2Spot
      containers:
      - name: processor
        image: batch-processor:latest
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi

                </pre>
              </code>
            </div>

            <h3>3. Cost Monitoring and Alerts:</h3>
            <div class="code-block">
              <code class="language-bash"> 
                <pre>

# Install KubeCost for cost visibility
helm repo add kubecost https://kubecost.github.io/cost-analyzer/
helm install kubecost kubecost/cost-analyzer \
  --namespace kubecost \
  --create-namespace \
  --set kubecostToken="YOUR_TOKEN" \
  --set prometheus.server.persistentVolume.size=32Gi \
  --set prometheus.alertmanager.persistentVolume.size=8Gi

# CloudWatch billing alert
aws cloudwatch put-metric-alarm \
  --alarm-name "EKS-Monthly-Cost-Alert" \
  --alarm-description "Alert when EKS monthly cost exceeds $500" \
  --metric-name EstimatedCharges \
  --namespace AWS/Billing \
  --statistic Maximum \
  --period 86400 \
  --threshold 500 \
  --comparison-operator GreaterThanThreshold \
  --dimensions Name=Currency,Value=USD \
  --evaluation-periods 1 \
  --alarm-actions arn:aws:sns:us-east-1:123456789012:billing-alerts
                </pre>
              </code>
            </div>

            <div class="highlight-box">
                <h3>Cost Optimization Best Practices:</h3>
                <ul>
                    <li><strong>Use Reserved Instances:</strong> 30-60% savings for predictable workloads</li>
                    <li><strong>Implement Pod Disruption Budgets:</strong> Safe spot instance termination</li>
                    <li><strong>Resource Limits:</strong> Prevent resource waste and noisy neighbors</li>
                    <li><strong>Fargate for Batch Jobs:</strong> Pay only for actual compute time</li>
                    <li><strong>Cluster Rightsizing:</strong> Regular review and optimization</li>
                </ul>
            </div>
        </div>

        <!-- Slide 14: Best Practices Summary -->
        <div class="slide">
            <h2>ğŸ¯ EKS Management Best Practices</h2>
            
            <h3>1. Operational Excellence:</h3>
            <div class="highlight-box">
                <h4>Infrastructure as Code:</h4>
                <ul>
                    <li><strong>Version Control:</strong> Store all configurations in Git repositories</li>
                    <li><strong>GitOps Workflow:</strong> Use ArgoCD or Flux for automated deployments</li>
                    <li><strong>Environment Parity:</strong> Consistent dev, staging, and production setups</li>
                    <li><strong>Backup Strategy:</strong> Regular ETCD backups and disaster recovery plans</li>
                </ul>
            </div>

            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Example ArgoCD Application manifest
apiVersion: v1
kind: Namespace
metadata:
  name: argocd
---

# GitOps with ArgoCD
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sample-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/k8s-manifests
    targetRevision: main
    path: sample-app
  destination:
    server: https://kubernetes.default.svc
    namespace: sample-app
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
                </pre>
              </code>
            </div>

            <h3>2. Security Best Practices:</h3>
            <div class="warning-box">
                <h4>Security Checklist:</h4>
                <ul>
                    <li><strong>Regular Updates:</strong> Keep EKS, nodes, and add-ons updated</li>
                    <li><strong>Image Scanning:</strong> Scan container images for vulnerabilities</li>
                    <li><strong>Least Privilege:</strong> Implement RBAC and IAM policies</li>
                    <li><strong>Network Segmentation:</strong> Use security groups and network policies</li>
                    <li><strong>Secrets Management:</strong> Never store secrets in code or configs</li>
                    <li><strong>Audit Logging:</strong> Enable and monitor all API server activities</li>
                </ul>
            </div>

            <h3>3. Monitoring and Observability:</h3>
            <div class="code-block">
              <code class="language-bash"> 
                <pre>
# Complete observability stack installation
# Prometheus + Grafana + AlertManager
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi \
  --set grafana.persistence.enabled=true \
  --set grafana.persistence.size=10Gi

# Install Jaeger for distributed tracing
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
helm install jaeger jaegertracing/jaeger \
  --namespace observability \
  --create-namespace \
  --set provisionDataStore.cassandra=false \
  --set provisionDataStore.elasticsearch=true \
  --set storage.type=elasticsearch
# Install Fluentd for log aggregation
helm repo add fluent https://fluent.github.io/helm-charts
helm install fluentd fluent/fluentd \
  --namespace logging \
  --create-namespace \
  --set fluentd.configMap.enabled=true \
  --set fluentd.configMap.data.fluent.conf='<source>
  @type tail
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  format json
</source>
<match kubernetes.**>
  @type elasticsearch
  host elasticsearch.logging.svc.cluster.local
  port 9200
  logstash_format true
  flush_interval 5s
</match>'
                </pre>
              </code>
            </div>

            <h3>4. Performance Optimization:</h3>
            <div class="highlight-box">
                <h4>Performance Best Practices:</h4>
                <ul>
                    <li><strong>Resource Requests/Limits:</strong> Set appropriate values for all containers</li>
                    <li><strong>Quality of Service:</strong> Use Guaranteed QoS for critical workloads</li>
                    <li><strong>Horizontal Pod Autoscaler:</strong> Scale based on custom metrics</li>
                    <li><strong>Node Affinity:</strong> Place pods on optimal nodes</li>
                    <li><strong>Pod Disruption Budgets:</strong> Ensure availability during maintenance</li>
                </ul>
            </div>
        </div>

        <!-- Slide 15: Kubernetes Objects Management for Pro Administrators -->
        <div class="slide">
            <h2>â˜¸ï¸ Pro-Level Kubernetes Objects Management</h2>
            
            <h3>1. Advanced Resource Management:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# ResourceQuota for namespace-level resource control
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    requests.cpu: "100"
    requests.memory: 200Gi
    limits.cpu: "200"
    limits.memory: 400Gi
    persistentvolumeclaims: "50"
    pods: "100"
    services: "20"
    secrets: "50"
    configmaps: "50"
    count/deployments.apps: "20"
    count/jobs.batch: "10"
---
# LimitRange for pod-level defaults and constraints
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limits
  namespace: production
spec:
  limits:
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "1Gi"
                </pre>
              </code>
            </div>

            <h3>2. Advanced Scheduling and Node Management:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Node Affinity and Taints for optimal scheduling
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-service
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: critical-service
      tier: frontend
  template:
    metadata:
      labels:
        app: critical-service
        tier: frontend
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-type
                operator: In
                values:
                - high-performance
      tolerations:
      - key: "critical"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      containers:
      - name: frontend
        image: critical-service:latest
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "1"
            memory: "1Gi"

# PriorityClass for critical workloads
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: critical-priority
value: 1000000
globalDefault: false
description: "Critical system workloads"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000
description: "High priority application workloads"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 1000
description: "Low priority batch workloads"

# Advanced Pod Disruption Budget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
  namespace: production
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: critical-service
      tier: frontend
  unhealthyPodEvictionPolicy: AlwaysAllow
  maxUnavailable: 1
                </pre>
              </code>
            </div>

            <h3>3. Custom Resource Definitions (CRDs) for Platform Management:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Application CRD for platform abstraction
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: applications.platform.company.com
spec:
  group: platform.company.com
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              replicas:
                type: integer
                minimum: 1
                maximum: 100
              image:
                type: string
              environment:
                type: string
                enum: ["dev", "staging", "production"]
              resources:
                type: object
                properties:
                  requests:
                    type: object
                    properties:
                      cpu:
                        type: string
                      memory:
                        type: string
                  limits:
                    type: object
                    properties:
                      cpu:
                        type: string
                      memory:
                        type: string
              monitoring:
                type: object
                properties:
                  enabled:
                    type: boolean
                  scrapeInterval:
                    type: string
              autoscaling:
                type: object
                properties:
                  enabled:
                    type: boolean
                  minReplicas:
                    type: integer
                  maxReplicas:
                    type: integer
                  targetCPU:
                    type: integer
                  targetMemory:
                    type: integer
          status:
            type: object
            properties:
              phase:
                type: string
              conditions:
                type: array
                items:
                  type: object
                  properties:
                    type:
                      type: string
                    status:
                      type: string
                    reason:
                      type: string
                    message:
                      type: string
  scope: Namespaced
  names:
    plural: applications
    singular: application
    kind: Application
    shortNames:
    - app
---
# Example Application CRD instance
apiVersion: platform.company.com/v1
kind: Application
metadata:
  name: sample-app
  namespace: production
spec:
  replicas: 3
  image: company/sample-app:latest
  environment: production
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1"
      memory: "1Gi"
  monitoring:
    enabled: true
    scrapeInterval: "30s"
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 5
    targetCPU: 80
    targetMemory: 70
status:
  phase: Running
  conditions:
  - type: Available
    status: "True"
    reason: AllPodsReady
    message: All pods are running and ready
                </pre>
              </code>
            </div>
        </div>

        <!-- Slide 16: GitOps with ArgoCD and Workflows -->
        <div class="slide">
            <h2>ğŸ”„ Enterprise GitOps with ArgoCD & Argo Workflows</h2>
            
            <h3>1. ArgoCD Multi-Cluster Setup:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# ArgoCD HA installation for production
apiVersion: v1
kind: Namespace
metadata:
  name: argocd
---
# ArgoCD ConfigMap for multi-cluster
apiVersion: v1
kind: ConfigMap
metadata:
  name: argocd-cm
  namespace: argocd
data:
  url: https://argocd.company.com
  oidc.config: |
    name: OIDC
    issuer: https://company.okta.com
    clientId: argocd-oidc
    clientSecret: $oidc.clientSecret
    requestedScopes: ["openid", "profile", "email", "groups"]
    requestedIDTokenClaims: {"groups": {"essential": true}}
  policy.default: role:readonly
  policy.csv: |
    p, role:admin, applications, *, */*, allow
    p, role:admin, clusters, *, *, allow
    p, role:admin, repositories, *, *, allow
    g, argocd-admins, role:admin
    g, platform-team, role:admin
  
  # Multi-cluster configuration
  cluster.config: |
    clusters:
      - name: dev-cluster
        server: https://dev-k8s-api.company.com
        config:
          awsRoleArn: arn:aws:iam::DEV_ACCOUNT:role/ArgoCD-Dev-Role
      - name: staging-cluster
        server: https://staging-k8s-api.company.com
        config:
          awsRoleArn: arn:aws:iam::STAGING_ACCOUNT:role/ArgoCD-Staging-Role
      - name: prod-cluster
        server: https://prod-k8s-api.company.com
        config:
          awsRoleArn: arn:aws:iam::PROD_ACCOUNT:role/ArgoCD-Prod-Role

# ApplicationSet for multi-environment deployment
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: microservices-appset
  namespace: argocd
spec:
  generators:
  - clusters:
      selector:
        matchLabels:
          environment: production
  - git:
      repoURL: https://github.com/company/k8s-manifests
      revision: HEAD
      directories:
      - path: applications/*
  template:
    metadata:
      name: '{{path.basename}}-{{name}}'
      namespace: argocd
      finalizers:
      - resources-finalizer.argocd.argoproj.io
    spec:
      project: default
      source:
        repoURL: https://github.com/company/k8s-manifests
        targetRevision: '{{metadata.labels.branch}}'
        path: '{{path}}/{{metadata.labels.environment}}'
        helm:
          valueFiles:
          - values-{{metadata.labels.environment}}.yaml
      destination:
        server: '{{server}}'
        namespace: '{{path.basename}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
        - CreateNamespace=true
        - PrunePropagationPolicy=foreground
        - PruneLast=true
        retry:
          limit: 5
          backoff:
            duration: 5s
            factor: 2
            maxDuration: 3m



                </pre>
              </code>
            </div>

            <h3>2. Argo Workflows for CI/CD Pipelines:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Argo Workflows installation with RBAC
apiVersion: v1
kind: ServiceAccount
metadata:
  name: argo-workflow
  namespace: argo-workflows
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: argo-workflow-executor
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "create", "delete"]
- apiGroups: ["argoproj.io"]
  resources: ["workflows", "workflows/finalizers"]
  verbs: ["get", "list", "watch", "update", "patch", "delete", "create"]

# Multi-stage CI/CD Workflow Template
apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: ci-cd-pipeline
  namespace: argo-workflows
spec:
  serviceAccountName: argo-workflow
  entrypoint: ci-cd-main
  volumeClaimTemplates:
  - metadata:
      name: workspace
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
  
  templates:
  - name: ci-cd-main
    dag:
      tasks:
      - name: git-clone
        template: git-clone
      - name: security-scan
        template: security-scan
        dependencies: [git-clone]
      - name: unit-tests
        template: unit-tests
        dependencies: [git-clone]
      - name: build-image
        template: build-image
        dependencies: [security-scan, unit-tests]
      - name: integration-tests
        template: integration-tests
        dependencies: [build-image]
      - name: deploy-dev
        template: deploy-environment
        arguments:
          parameters:
          - name: environment
            value: "dev"
        dependencies: [integration-tests]
      - name: e2e-tests
        template: e2e-tests
        dependencies: [deploy-dev]
      - name: deploy-staging
        template: deploy-environment
        arguments:
          parameters:
          - name: environment
            value: "staging"
        dependencies: [e2e-tests]
      - name: performance-tests
        template: performance-tests
        dependencies: [deploy-staging]
      - name: deploy-prod-approval
        template: approval-gate
        dependencies: [performance-tests]
      - name: deploy-production
        template: deploy-environment
        arguments:
          parameters:
          - name: environment
            value: "production"
        dependencies: [deploy-prod-approval]

  - name: git-clone
    container:
      image: alpine/git:latest
      command: [sh, -c]
      args: ["git clone {{workflow.parameters.repo-url}} /workspace && cd /workspace && git checkout {{workflow.parameters.revision}}"]
      volumeMounts:
      - name: workspace
        mountPath: /workspace

  - name: security-scan
    container:
      image: aquasec/trivy:latest
      command: [sh, -c]
      args: ["trivy fs --security-checks vuln,config /workspace --format json --output /workspace/security-report.json"]
      volumeMounts:
      - name: workspace
        mountPath: /workspace

  - name: build-image
    container:
      image: gcr.io/kaniko-project/executor:latest
      args:
      - --context=/workspace
      - --dockerfile=/workspace/Dockerfile
      - --destination={{workflow.parameters.image-repo}}:{{workflow.parameters.image-tag}}
      - --cache=true
      - --cache-ttl=24h
      volumeMounts:
      - name: workspace
        mountPath: /workspace
      - name: docker-config
        mountPath: /kaniko/.docker

                </pre>
              </code>
            </div>
        </div>

        <!-- Slide 17: Terraform Multi-Environment Management -->
        <div class="slide">
            <h2>ğŸ—ï¸ Terraform Multi-Environment Infrastructure</h2>
            
            <h3>1. Terraform Workspace Structure:</h3>
            <div class="code-block">
              <code class="language-text"> 
                <pre>
# Directory structure for multi-environment Terraform
infrastructure/
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”œâ”€â”€ terraform.tfvars
â”‚   â”‚   â””â”€â”€ backend.tf
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”œâ”€â”€ terraform.tfvars
â”‚   â”‚   â””â”€â”€ backend.tf
â”‚   â””â”€â”€ production/
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ variables.tf
â”‚       â”œâ”€â”€ terraform.tfvars
â”‚       â””â”€â”€ backend.tf
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ eks-cluster/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”‚   â””â”€â”€ versions.tf
â”‚   â”œâ”€â”€ vpc/
â”‚   â”œâ”€â”€ rds/
â”‚   â”œâ”€â”€ elasticache/
â”‚   â””â”€â”€ monitoring/
â””â”€â”€ shared/
    â”œâ”€â”€ policies/
    â”œâ”€â”€ roles/
    â””â”€â”€ common-tags.tf
                </pre>
              </code>
            </div>

            <h3>2. Environment-Specific Configurations:</h3>
            <div class="code-block">
              <code class="language-hcl"> 
                <pre>


# environments/production/main.tf
terraform {
  required_version = ">= 1.5"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.23"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.11"
    }
  }
  
  backend "s3" {
    bucket         = "company-terraform-state-prod"
    key            = "eks/production/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "terraform-state-lock-prod"
  }
}

# Provider configurations
provider "aws" {
  region = var.aws_region
  
  assume_role {
    role_arn = "arn:aws:iam::${var.prod_account_id}:role/TerraformExecutionRole"
  }
  
  default_tags {
    tags = local.common_tags
  }
}

# Local values for environment-specific configurations
locals {
  environment = "production"
  cluster_name = "production-eks-${var.cluster_version}"
  
  common_tags = {
    Environment   = local.environment
    ManagedBy     = "Terraform"
    Project       = var.project_name
    Owner         = var.team_name
    CostCenter    = var.cost_center
    Compliance    = "SOC2-PCI"
    Backup        = "Required"
    Monitoring    = "Enhanced"
  }
  
  # Production-specific scaling parameters
  cluster_config = {
    min_nodes     = 10
    max_nodes     = 500
    desired_nodes = 20
    instance_types = ["m5.2xlarge", "m5.4xlarge", "c5.2xlarge"]
    capacity_type  = "ON_DEMAND"
  }
  
  spot_config = {
    min_nodes     = 5
    max_nodes     = 1000
    desired_nodes = 50
    instance_types = ["m5.large", "m5.xlarge", "m4.large", "m4.xlarge"]
    capacity_type  = "SPOT"
  }
}

# VPC Module
module "vpc" {
  source = "../../modules/vpc"
  
  environment    = local.environment
  cluster_name   = local.cluster_name
  vpc_cidr       = var.vpc_cidr
  azs           = var.availability_zones
  
  # Production requires more subnets for scalability
  private_subnets = [
    "10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24",
    "10.0.11.0/24", "10.0.12.0/24", "10.0.13.0/24"
  ]
  public_subnets = [
    "10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"
  ]
  database_subnets = [
    "10.0.201.0/24", "10.0.202.0/24", "10.0.203.0/24"
  ]
  
  enable_nat_gateway     = true
  enable_vpn_gateway     = false
  enable_dns_hostnames   = true
  enable_dns_support     = true
  
  # VPC Flow Logs for security
  enable_flow_log                      = true
  flow_log_destination_type           = "cloud-watch-logs"
  flow_log_log_format                 = "${version} ${account-id} ${interface-id} ${srcaddr} ${dstaddr} ${srcport} ${dstport} ${protocol} ${packets} ${bytes} ${windowstart} ${windowend} ${action} ${flowlogstatus}"
  
  tags = local.common_tags
}

                </pre>
              </code>
            </div>

            <h3>2. Scalable EKS Cluster Module:</h3>
            <div class="code-block">
              <code class="language-hcl"> 
                <pre>
# modules/eks-cluster/main.tf - Production-grade EKS
resource "aws_eks_cluster" "main" {
  name     = var.cluster_name
  role_arn = aws_iam_role.cluster_role.arn
  version  = var.kubernetes_version
  
  vpc_config {
    subnet_ids              = var.subnet_ids
    endpoint_private_access = true
    endpoint_public_access  = true
    public_access_cidrs     = var.public_access_cidrs
    security_group_ids      = [aws_security_group.cluster_sg.id]
  }
  
  # Enhanced logging for production
  enabled_cluster_log_types = [
    "api", "audit", "authenticator", 
    "controllerManager", "scheduler"
  ]
  
  encryption_config {
    provider {
      key_arn = aws_kms_key.eks_encryption.arn
    }
    resources = ["secrets"]
  }
  
  # Production clusters need enhanced monitoring
  kubernetes_network_config {
    service_ipv4_cidr = var.service_ipv4_cidr
    ip_family         = "ipv4"
  }
  
  depends_on = [
    aws_iam_role_policy_attachment.cluster_AmazonEKSClusterPolicy,
    aws_iam_role_policy_attachment.cluster_AmazonEKSVPCResourceController,
    aws_cloudwatch_log_group.cluster_logs,
  ]
  
  tags = merge(var.tags, {
    "kubernetes.io/cluster/${var.cluster_name}" = "owned"
  })
}

# Multiple node groups for different workload types
resource "aws_eks_node_group" "system" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.cluster_name}-system"
  node_role_arn   = aws_iam_role.node_role.arn
  subnet_ids      = var.private_subnet_ids
  
  # System workloads node group configuration
  capacity_type  = "ON_DEMAND"
  instance_types = ["m5.large"]
  
  scaling_config {
    desired_size = 3
    max_size     = 10
    min_size     = 3
  }
  
  update_config {
    max_unavailable = 1
  }
  
  # Taints for system workloads
  taint {
    key    = "CriticalAddonsOnly"
    value  = "true"
    effect = "NO_SCHEDULE"
  }
  
  labels = {
    "node-type"    = "system"
    "workload"     = "system-critical"
    "environment"  = var.environment
  }
  
  tags = merge(var.tags, {
    "kubernetes.io/cluster/${var.cluster_name}" = "owned"
    "k8s.io/cluster-autoscaler/enabled"         = "true"
    "k8s.io/cluster-autoscaler/${var.cluster_name}" = "owned"
  })
}

resource "aws_eks_node_group" "general_purpose" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.cluster_name}-general"
  node_role_arn   = aws_iam_role.node_role.arn
  subnet_ids      = var.private_subnet_ids
  
  capacity_type  = "ON_DEMAND"
  instance_types = var.general_instance_types
  
  scaling_config {
    desired_size = var.general_desired_size
    max_size     = var.general_max_size
    min_size     = var.general_min_size
  }
  
  # Advanced configuration for production
  remote_access {
    ec2_ssh_key               = var.key_name
    source_security_group_ids = [aws_security_group.node_sg.id]
  }
  
  labels = {
    "node-type"     = "general-purpose"
    "workload"      = "application"
    "environment"   = var.environment
  }
}

resource "aws_eks_node_group" "spot" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.cluster_name}-spot"
  node_role_arn   = aws_iam_role.node_role.arn
  subnet_ids      = var.private_subnet_ids
  
  capacity_type  = "SPOT"
  instance_types = var.spot_instance_types
  
  scaling_config {
    desired_size = var.spot_desired_size
    max_size     = var.spot_max_size
    min_size     = var.spot_min_size
  }
  
  # Spot instance specific taints
  taint {
    key    = "spotInstance"
    value  = "true"
    effect = "NO_SCHEDULE"
  }
  
  labels = {
    "node-type"     = "spot"
    "workload"      = "batch-processing"
    "lifecycle"     = "Ec2Spot"
    "environment"   = var.environment
  }
}
  
                </pre>
              </code>
            </div>
        </div>

        <!-- Slide 18: Mega-Scale Infrastructure Patterns -->
        <div class="slide">
            <h2>ğŸŒ Mega-Scale Infrastructure: 1000+ Clusters, 10M+ Pods</h2>
            
            <h3>1. Cluster Federation and Multi-Region Architecture:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Cluster Federation using Admiral and Skupper
# Regional cluster distribution for global scale
regions:
  - name: us-east-1
    clusters: 200
    capacity: 2M-pods
    role: primary
  - name: us-west-2
    clusters: 200
    capacity: 2M-pods
    role: secondary
  - name: eu-west-1
    clusters: 150
    capacity: 1.5M-pods
    role: regional-hub
  - name: ap-southeast-1
    clusters: 150
    capacity: 1.5M-pods
    role: regional-hub
  - name: edge-locations
    clusters: 400
    capacity: 3M-pods
    role: edge-compute

# Hierarchical cluster management
apiVersion: config.admiral.io/v1
kind: AdmiralConfig
metadata:
  name: global-mesh-config
spec:
  # Global traffic management for 1000+ clusters
  dependencyNamespaceLabel: "admiral.io/env"
  workloadSidecarUpdate: "enabled"
  meshNetworks:
    network1:
      endpoints:
      - fromRegistry: us-east-clusters
      gateways:
      - address: east-gateway.company.com
        port: 15443
    network2:
      endpoints:
      - fromRegistry: us-west-clusters
      gateways:
      - address: west-gateway.company.com
        port: 15443
  
  # Auto-discovery for massive scale
  clusterRegistries:
  - name: us-east-registry
    endpoint: https://east-registry.company.com
    secretName: cluster-registry-secret
  - name: us-west-registry
    endpoint: https://west-registry.company.com
    secretName: cluster-registry-secret

# Cluster API for automated cluster lifecycle
apiVersion: cluster.x-k8s.io/v1beta1
kind: ClusterClass
metadata:
  name: eks-production-template
spec:
  controlPlane:
    ref:
      apiVersion: controlplane.cluster.x-k8s.io/v1beta2
      kind: AWSManagedControlPlaneTemplate
      name: production-control-plane
  infrastructure:
    ref:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
      kind: AWSClusterTemplate
      name: production-cluster
  workers:
    machineDeployments:
    - class: default-worker
      template:
        bootstrap:
          ref:
            apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
            kind: EKSConfigTemplate
            name: production-worker-bootstrap
        infrastructure:
          ref:
            apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
            kind: AWSMachineTemplate
            name: production-worker-machine
                </pre>
              </code>
            </div>

            <h3>2. Massive Scale Networking with Cilium:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Cilium installation for massive scale
apiVersion: v1
kind: Namespace
metadata:
  name: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: cilium
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium
  template:
    metadata:
      labels:        
        k8s-app: cilium
    spec:
      serviceAccountName: cilium
      containers:
      - name: cilium-agent
        image: cilium/cilium:v1.12.0
        args:
        - "cilium-agent"
        - "--config-dir=/tmp/cilium-config"
        - "--k8s-namespace=kube-system"
        - "--k8s-require-ipv4-pod-cidr=true"
        - "--enable-ipv4=true"
        - "--enable-ipv6=false"
        - "--enable-bpf-masquerade=true"
        - "--enable-local-redirect-policy=true"
        - "--enable-bandwidth-manager=true"
        - "--enable-bbr=true"
        - "--enable-policy=true"
        - "--policy-enforcement=default"
        - "--enable-l7-proxy=true"
        - "--enable-hubble=true"
        - "--hubble-metrics-server=:9965"
        - "--hubble-metrics=dns,drop,tcp,flow,icmp,http"
        - "--ipam=kubernetes"
        - "--auto-direct-node-routes=true"
        - "--enable-local-node-route=false"
        - "--cluster-pool-ipv4-cidr=CIDR"
        - "--cluster-pool-ipv4-mask-size=16"
        - "--cluster-mesh-config=true"
        - "--cluster-id=1"
        - "--cluster-name=production-us-east-1-cluster-001"
        - "--enable-endpoint-health-checking=false"
        - "--enable-health-check-nodeport=false"
        volumeMounts:
        - name: cilium-config
          mountPath: /tmp/cilium-config
      volumes:
      - name: cilium-config
        configMap:
          name: cilium-config
      - name: cilium-bpf
        hostPath:

# Cilium configuration for 10M+ pods
apiVersion: v1
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: kube-system
data:
  # Enable eBPF datapath for maximum performance
  datapath-mode: "veth"
  enable-bpf-masquerade: "true"
  enable-ip-masq-agent: "false"
  
  # Massive scale network configuration
  cluster-pool-ipv4-cidr: "10.0.0.0/8"
  cluster-pool-ipv4-mask-size: "16"
  
  # Enable Cluster Mesh for multi-cluster networking
  cluster-mesh-config: "true"
  cluster-id: "1"
  cluster-name: "production-us-east-1-cluster-001"
  
  # Performance optimizations for high pod density
  enable-local-redirect-policy: "true"
  enable-bandwidth-manager: "true"
  enable-bbr: "true"
  
  # Security features at scale
  enable-policy: "default"
  policy-enforcement: "default"
  enable-l7-proxy: "true"
  
  # Observability for massive scale
  enable-hubble: "true"
  hubble-metrics-server: ":9965"
  hubble-metrics: "dns,drop,tcp,flow,icmp,http"
  
  # Resource optimizations
  k8s-require-ipv4-pod-cidr: "true"
  enable-endpoint-health-checking: "false"
  enable-health-check-nodeport: "false"
  
  # IPAM configuration for scale
  ipam: "kubernetes"
  auto-direct-node-routes: "true"
  enable-local-node-route: "false"

# Network Policy for micro-segmentation at scale
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: global-default-deny
spec:
  endpointSelector: {}
  ingress: []
  egress:
  - toEndpoints:
    - matchLabels:
        k8s:io.kubernetes.pod.namespace: kube-system
        k8s:app: coredns
    toPorts:
    - ports:
      - port: "53"
        protocol: UDP
      - port: "53"
        protocol: TCP

# ClusterMesh configuration for cross-cluster communication
apiVersion: v1
kind: Secret
metadata:
  name: clustermesh-apiserver-remote-cert
  namespace: kube-system
data:
  tls.crt: # Remote cluster certificate
  tls.key: # Remote cluster key
  ca.crt: # CA certificate
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: clustermesh-config
  namespace: kube-system
data:
  config.yaml: |
    clusters:
    - name: cluster-us-east-1-001
      address: clustermesh-apiserver.us-east-1.company.com
      port: 2379
    - name: cluster-us-west-2-001
      address: clustermesh-apiserver.us-west-2.company.com
      port: 2379
    - name: cluster-eu-west-1-001
      address: clustermesh-apiserver.eu-west-1.company.com
      port: 2379
                </pre>
              </code>
            </div>

            <h3>3. Control Plane Scaling and ETCD Optimization:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Control Plane scaling for 1000+ clusters
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-controller-manager-config
  namespace: kube-system
data:
  # AWS EKS Controller Manager configuration
  cluster-name: "production-cluster"
  leader-elect: "true"
  controllers: "*,node,replicaset,daemonset,statefulset,job,
    cronjob,endpoint,serviceaccount,namespace,persistentvolumeclaim"
  concurrent-gc-syncs: "10"  # Increase for massive scale
  node-monitor-grace-period: "40s"  # Faster detection of node failures
  node-monitor-period: "10s"  # More frequent node checks
  pod-eviction-timeout: "30s"  # Quicker eviction for unresponsive pods
  resource-eviction-rate: "0.1"  # Higher rate for large clusters
  eviction-hard: "memory.available < 100Mi,nodefs.available < 10%,imagefs.available < 10%"
  eviction-soft: "memory.available < 200Mi,nodefs.available < 20%,imagefs.available < 20%"
  eviction-soft-grace-period: "memory.available=1m,nodefs.available=1m,imagefs.available=1m"
  eviction-max-pod-grace-period: "30s"
  horizontal-pod-autoscaler-sync-period: "10s"
  horizontal-pod-autoscaler-downscale-stabilization: "30s"
  horizontal-pod-autoscaler-upscale-stabilization: "30s"
  horizontal-pod-autoscaler-initial-readiness-delay: "10s"

# ETCD configuration for massive scale
apiVersion: v1
kind: ConfigMap
metadata:
  name: etcd-config
  namespace: kube-system
data:
# ETCD configuration for massive scale (AWS managed, conceptual)
etcd_cluster_config:
  # AWS EKS manages ETCD, but understanding the scale requirements
  cluster_size: 5  # AWS uses 3-5 nodes for HA
  disk_type: "io2"  # High IOPS for large clusters
  disk_size: "500GB"  # Larger for extensive object storage
  instance_type: "r5.2xlarge"  # Memory-optimized for large key-value stores
  
  # Performance tuning parameters (conceptual for AWS managed)
  heartbeat_interval: "100ms"
  election_timeout: "1000ms"
  snapshot_count: 100000
  
  # Backup strategy for massive scale
  backup_frequency: "6h"
  backup_retention: "30d"
  cross_region_backup: true

  backup_storage: "s3://company-etcd-backups/production/"
  backup_encryption: "AES256"
  backup_compression: "gzip"
  backup_notification: "sns://etcd-backup-notifications"
  backup_tags: "Environment=Production,ManagedBy=Terraform,Project=Kubernetes"
  backup_schedule: "cron(0 */6 * * ? *)"  # Every 6 hours
  backup_lifecycle_policy: "s3://company-etcd-backups/production/lifecycle.json"
  lifecycle_policy: |
    {
      "Rules": [
        {
          "ID": "ExpireOldBackups",
          "Status": "Enabled",
          "Prefix": "",
          "Expiration": {
            "Days": 30
          },
          "NoncurrentVersionExpiration": {  
            "NoncurrentDays": 30
          }
        }
      ]
    }

# API Server tuning for high load
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-apiserver-config
  namespace: kube-system
data:
  # AWS EKS API server configuration (via cluster config)
  audit-policy.yaml: |
    apiVersion: audit.k8s.io/v1
    kind: Policy
    rules:
    - level: None
      resources:
      - group: ""
        resources: ["events"]
    - level: None
      resources:
      - group: ""
        resources: ["nodes", "pods", "services"]
    - level: Metadata
      resources:
      - group: ""
        resources: ["secrets", "configmaps"]
    - level: RequestResponse
      resources:
      - group: ""
        resources: ["deployments", "statefulsets", "daemonsets"]
    - level: Request
      resources:
      - group: ""
        resources: ["namespaces", "serviceaccounts"]
    - level: RequestResponse
      resources:
      - group: "apps"
        resources: ["replicasets", "jobs", "cronjobs"]
    - level: RequestResponse
      resources:
      - group: "batch"
        resources: ["jobs", "cronjobs"]
    - level: RequestResponse
      resources:
      - group: "extensions"
        resources: ["ingresses"]
    - level: RequestResponse
      resources:
      - group: "networking.k8s.io"
        resources: ["ingresses"]
    - level: RequestResponse
      resources:
      - group: "rbac.authorization.k8s.io"
        resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]
    - level: RequestResponse
      resources:
      - group: "storage.k8s.io"
        resources: ["storageclasses", "volumeattachments"]
    - level: RequestResponse
      resources:
      - group: "apiextensions.k8s.io"
        resources: ["customresourcedefinitions"]
    - level: RequestResponse
      resources:
      - group: "admissionregistration.k8s.io"
        resources: ["mutatingwebhookconfigurations", "validatingwebhookconfigurations"]
    - level: RequestResponse
      resources:
      - group: "policy"
        resources: ["poddisruptionbudgets"]
    - level: RequestResponse
      resources:
      - group: "autoscaling"
        resources: ["horizontalpodautoscalers"]
    - level: RequestResponse
      resources:
      - group: "metrics.k8s.io"
        resources: ["pods", "nodes"]
   
                </pre>
              </code>
            
        </div>
            <h3>1. Service Mesh Integration:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Install Istio service mesh
curl -L https://istio.io/downloadIstio | sh -
export PATH=$PWD/istio-1.19.0/bin:$PATH

istioctl install --set values.defaultRevision=default
kubectl label namespace sample-app istio-injection=enabled

# Istio Gateway for traffic management
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: sample-app-gateway
  namespace: sample-app
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - myapp.example.com
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: myapp-tls-secret
    hosts:
    - myapp.example.com
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: sample-app-vs
  namespace: sample-app
spec:
  hosts:
  - myapp.example.com
  gateways:
  - sample-app-gateway
  http:
  - match:
    - uri:
        prefix: /api
    route:
    - destination:
        host: backend-service
        port:
          number: 80
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
  - route:
    - destination:
        host: frontend-service
        port:
          number: 80
    corsPolicy:
      allowOrigins:
      - exact: "https://myapp.example.com"
      allowMethods: "GET, POST, PUT, DELETE, OPTIONS"
      allowHeaders:
      - "content-type"
      maxAge: "24h"
      allowCredentials: true
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: sample-app-dr
  namespace: sample-app
spec:
  host: myapp.example.com
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN
    connectionPool:
      tcp:
        maxConnections: 1000
      http:
        maxRequestsPerConnection: 100
    outlierDetection:
      consecutiveErrors: 5
      interval: 1s
      baseEjectionTime: 30s
    tls:
      mode: SIMPLE
      credentialName: myapp-tls-secret
      minProtocolVersion: TLSV1_2
      maxProtocolVersion: TLSV1_3
---
# Istio Sidecar configuration for massive scale
apiVersion: networking.istio.io/v1alpha3
kind: Sidecar
metadata:
  name: sample-app-sidecar
  namespace: sample-app
spec:
  egress:
  - hosts:
    - "./*"
  - hosts:
    - "istio-system/*"
  - hosts:
    - "kube-system/*"
  ingress:
  - defaultEndpoint: " 
                </pre>
              </code>
            </div>

            <h3>2. Multi-Cluster Management:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# Cluster API for multi-cluster management
# Install cluster-api components
clusterctl init --infrastructure aws

# Create workload cluster
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: workload-cluster-1
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks: ["192.168.0.0/16"]
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: AWSCluster
    name: workload-cluster-1
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: workload-cluster-1-control-plane

# Admiral for multi-cluster service mesh
kubectl apply -f https://raw.githubusercontent.com/istio-ecosystem/admiral/master/install/yaml/admiral.yaml
                </pre>
              </code>
            </div>

            <h3>3. Edge Computing with EKS Anywhere:</h3>
            <div class="code-block">
              <code class="language-yaml"> 
                <pre>
# EKS Anywhere cluster configuration
apiVersion: anywhere.eks.amazonaws.com/v1alpha1
kind: Cluster
metadata:
  name: edge-cluster
spec:
  kubernetesVersion: "1.28"
  clusterNetwork:
    cniConfig:
      cilium: {}
  controlPlaneConfiguration:
    count: 3
    endpoint:
      host: "10.0.1.10"
    machineGroupRef:
      kind: VSphereMachineConfig
      name: edge-control-plane
  workerNodeGroupConfigurations:
  - count: 5
    machineGroupRef:
      kind: VSphereMachineConfig
      name: edge-worker-nodes
    name: worker-nodes
  datacenterRef:
    kind: VSphereDatacenterConfig
    name: edge-datacenter

                </pre>
              </code>
            </div>

            <h3>4. Future Roadmap Considerations:</h3>
            <div class="highlight-box">
                <h4>Emerging Technologies:</h4>
                <ul>
                    <li><strong>WebAssembly (WASM):</strong> Lightweight, secure container alternatives</li>
                    <li><strong>eBPF:</strong> Advanced networking and security capabilities</li>
                    <li><strong>Serverless Containers:</strong> AWS Lambda for Kubernetes integration</li>
                    <li><strong>AI/ML Workloads:</strong> GPU nodes and specialized operators</li>
                    <li><strong>Edge Computing:</strong> EKS Anywhere and edge locations</li>
                    <li><strong>Sustainability:</strong> Green computing and carbon optimization</li>
                </ul>
            </div>

            <div class="warning-box">
                <h4>Migration and Modernization:</h4>
                <ul>
                    <li><strong>Gradual Migration:</strong> Strangler fig pattern for legacy applications</li>
                    <li><strong>Hybrid Cloud:</strong> Multi-cloud and on-premises integration</li>
                    <li><strong>Compliance:</strong> SOC2, HIPAA, PCI-DSS considerations</li>
                    <li><strong>Disaster Recovery:</strong> Cross-region and cross-cloud strategies</li>
                </ul>
            </div>

            <div style="text-align: center; margin-top: 50px; padding: 30px; background: linear-gradient(135deg, #667eea, #764ba2); border-radius: 15px; color: white;">
                <h2>ğŸ‰ Thank You!</h2>
                <p style="font-size: 1.2em;">Advanced AWS EKS Workshop Complete</p>
                <p>Ready for Production-Grade Kubernetes on AWS!</p>
                <div style="margin-top: 20px;">
                    <p><strong>Next Steps:</strong></p>
                    <ul style="list-style: none; padding: 0;">
                        <li>ğŸ”§ Implement hands-on exercises</li>
                        <li>ğŸ“Š Set up monitoring and alerting</li>
                        <li>ğŸ” Apply security best practices</li>
                        <li>ğŸ’° Optimize costs continuously</li>
                        <li>ğŸš€ Scale your applications confidently</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">â† Previous</button>
        <button class="nav-btn" onclick="nextSlide()">Next â†’</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        
        document.getElementById('total-slides').textContent = totalSlides;
        
        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = (n + totalSlides) % totalSlides;
            slides[currentSlide].classList.add('active');
            document.getElementById('current-slide').textContent = currentSlide + 1;
        }
        
        function nextSlide() {
            showSlide(currentSlide + 1);
        }
        
        function previousSlide() {
            showSlide(currentSlide - 1);
        }
        
        // Keyboard navigation
        document.addEventListener('keydown', function(e) {
            if (e.key === 'ArrowRight' || e.key === ' ') {
                nextSlide();
            } else if (e.key === 'ArrowLeft') {
                previousSlide();
            } else if (e.key >= '1' && e.key <= '9') {
                const slideNumber = parseInt(e.key) - 1;
                if (slideNumber < totalSlides) {
                    showSlide(slideNumber);
                }
            }
        });
        
        // Auto-resize text for mobile
        function adjustFontSize() {
            const container = document.querySelector('.presentation-container');
            const containerWidth = container.offsetWidth;
            
            if (containerWidth < 768) {
                document.body.style.fontSize = '14px';
                document.querySelectorAll('.slide').forEach(slide => {
                    slide.style.padding = '20px 30px';
                });
            } else {
                document.body.style.fontSize = '16px';
                document.querySelectorAll('.slide').forEach(slide => {
                    slide.style.padding = '40px 60px';
                });
            }
        }
        
        window.addEventListener('resize', adjustFontSize);
        adjustFontSize();
        
        // Progress bar
        function updateProgress() {
            const progress = ((currentSlide + 1) / totalSlides) * 100;
            document.querySelector('.slide-counter').style.background = 
                `linear-gradient(90deg, #3498db ${progress}%, rgba(52, 73, 94, 0.8) ${progress}%)`;
        }
        
        // Update progress on slide change
        const originalShowSlide = showSlide;
        showSlide = function(n) {
            originalShowSlide(n);
            updateProgress();
        };
        
        updateProgress();
    </script>
</body>
</html>
                    